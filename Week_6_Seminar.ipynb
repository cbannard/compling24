{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RByeKCfdaSZ7"
      },
      "source": [
        "# LELA32052 Computational Linguistics Week 6\n",
        "\n",
        "This week we are going to a) take a look at multiclass classification and b)explore a few different neural network structures for handling sequences - sentences and sequences of letters. Our focus will be on classifying (applying a single label to a sequence) but we will also look briefly at generation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiclass classification problems\n",
        "\n",
        "While logistic regression is great for binary classification tasks, many classification problems have more than two possible outcomes.  We can simulate such a situation as follows. I have just generalised sentiment analysis to a three class problem - negative, neutral and positive.\n",
        "\n"
      ],
      "metadata": {
        "id": "58xkbBkUzkVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (1, 3)\n",
        "w2_center = (3, 1)\n",
        "w3_center = (1, 1)\n",
        "w4_center = (3, 3)\n",
        "\n",
        "x=np.concatenate((np.random.normal(loc=w1_center,size=(20,2)),np.random.normal(loc=w2_center,size=(20,2)),np.random.normal(loc=w3_center,size=(10,2)),np.random.normal(loc=w4_center,size=(10,2))))\n",
        "labs=np.repeat([0,1,2],[20,20,20],axis=0)\n",
        "y=np.repeat(np.diag((1,1,1)),[20,20,20],axis=0)\n",
        "x=x.T\n",
        "y=y.T"
      ],
      "metadata": {
        "id": "k-yx9h_40DIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x[0][labs==0], x[1][labs==0], marker='*', s=100)\n",
        "plt.scatter(x[0][labs==1], x[1][labs==1], marker='o', s=100)\n",
        "plt.scatter(x[0][labs==2], x[1][labs==2], marker='x', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))\n"
      ],
      "metadata": {
        "id": "I3japJAV50NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Softmax\n",
        "In such circumstances we need to use multinomial logistic (aka softmax) regression.\n",
        "\n",
        "In logistic regression we take the dot product between our feature vector for each data point and our weight vector. We then add the bias to give us a single z value which we feed through the sigmoid function. We can have only one z values because there are only two outcomes and the following relationship holds:\n",
        "p(y=0|x) = 1-p(y=1|x)\n",
        "\n",
        "In multinomial regression we instead have a z value for each of our possible outcomes. We can use these collectively to calculate probabilties for each of our possible outcomes. For example if we had three possible outcomes, 0, 1 or 2 then we would calculate their probabilities as follows:\n",
        "\n",
        "$p(y=0|x) = \\frac{exp(z_{0})}{\\sum_{i,N} exp(z_i)}$ \\\\\n",
        "$p(y=1|x) = \\frac{exp(z_{1})}{\\sum_{i,N} exp(z_i)}$ \\\\\n",
        "$p(y=2|x) = \\frac{exp(z_{2})}{\\sum_{i,N} exp(z_i)}$ \\\\\n"
      ],
      "metadata": {
        "id": "v6QacNfX8TPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "n_iters = 2500\n",
        "num_features=2\n",
        "num_classes=3\n",
        "num_samples = len(y[0])\n",
        "weights = np.random.rand(num_classes,num_features)\n",
        "bias=np.zeros(num_classes)\n",
        "lr=0.1\n",
        "logistic_loss=[]\n",
        "z=np.zeros((num_samples,num_classes))\n",
        "q=np.zeros((num_samples,num_classes))\n",
        "\n",
        "for i in range(n_iters):\n",
        "    z[:,0]=x[0]*weights[0,0] + x[1]*weights[0,1] + bias[0]\n",
        "    z[:,1]=x[0]*weights[1,0] + x[1]*weights[1,1] + bias[1]\n",
        "    z[:,2]=x[0]*weights[2,0] + x[1]*weights[2,1] + bias[2]\n",
        "\n",
        "    q[:,0] = np.exp(z[:,0])/np.exp(z).sum(axis=1)\n",
        "    q[:,1] = np.exp(z[:,1])/np.exp(z).sum(axis=1)\n",
        "    q[:,2] = np.exp(z[:,2])/np.exp(z).sum(axis=1)\n",
        "\n",
        "    loss = sum(-(y[0]*np.log2(q[:,0])+(1-y[0])*np.log2(1-q[:,0])))/num_samples\n",
        "    loss += sum(-(y[1]*np.log2(q[:,1])+(1-y[1])*np.log2(1-q[:,1])))/num_samples\n",
        "    loss += sum(-(y[2]*np.log2(q[:,2])+(1-y[2])*np.log2(1-q[:,2])))/num_samples\n",
        "    logistic_loss.append(loss)\n",
        "\n",
        "    dw01 = sum(x[0]*(q[:,0]-y[0]))/num_samples # derivative with regards to weight 1 for outcome 0\n",
        "    dw02 = sum(x[1]*(q[:,0]-y[0]))/num_samples # derivative with regards to weight 2 for outcome 0\n",
        "\n",
        "    dw11 = sum(x[0]*(q[:,1]-y[1]))/num_samples # derivative with regards to weight 1 for outcome 1\n",
        "    dw12 = sum(x[1]*(q[:,1]-y[1]))/num_samples # derivative with regards to weight 2 for outcome 1\n",
        "\n",
        "    dw21 = sum(x[0]*(q[:,2]-y[2]))/num_samples # derivative with regards to weight 1 for outcome 2\n",
        "    dw22 = sum(x[1]*(q[:,2]-y[2]))/num_samples # derivative with regards to weight 2 for outcome 2\n",
        "\n",
        "    db0 = sum(q[:,0]-y[0])/num_samples # derivative with regards to bias for outcome 0\n",
        "    db1 = sum(q[:,1]-y[1])/num_samples # derivative with regards to bias for outcome 1\n",
        "    db2 = sum(q[:,2]-y[2])/num_samples # derivative with regards to bias for outcome 2\n",
        "\n",
        "    weights[0,0] = weights[0,0] - dw01*lr\n",
        "    weights[0,1] = weights[0,1] - dw02*lr\n",
        "\n",
        "    weights[1,0] = weights[1,0] - dw11*lr\n",
        "    weights[1,1] = weights[1,1] - dw12*lr\n",
        "\n",
        "    weights[2,0] = weights[2,0] - dw21*lr\n",
        "    weights[2,1] = weights[2,1] - dw22*lr\n",
        "\n",
        "    bias[0] = bias[0] - db0*lr\n",
        "    bias[1] = bias[1] - db1*lr\n",
        "    bias[2] = bias[2] - db2*lr\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "mtUjZzhE60CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing model predictions to real data"
      ],
      "metadata": {
        "id": "NLZ7DdV_5Vca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x[0][labs==0], x[1][labs==0], marker='*', s=100)\n",
        "plt.scatter(x[0][labs==1], x[1][labs==1], marker='o', s=100)\n",
        "plt.scatter(x[0][labs==2], x[1][labs==2], marker='x', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))\n"
      ],
      "metadata": {
        "id": "gp3guPtYqHDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labs=np.array([np.argmax(i) for i in q])\n",
        "plt.scatter(x[0][labs==0], x[1][labs==0], marker='*', s=100)\n",
        "plt.scatter(x[0][labs==1], x[1][labs==1], marker='o', s=100)\n",
        "plt.scatter(x[0][labs==2], x[1][labs==2], marker='x', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))"
      ],
      "metadata": {
        "id": "Gznq1cKn5Taq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate accuracy for our multiclass classifier"
      ],
      "metadata": {
        "id": "rNII8LyUvjH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean([int(np.argmax(x) == np.argmax(y.T[i]))  for i,x in enumerate(q)])"
      ],
      "metadata": {
        "id": "HOddLo0SyNyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass classification with a real dataset"
      ],
      "metadata": {
        "id": "6LKW2ZpPz24f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMpDGWm25RkV"
      },
      "source": [
        "### Classifying surnames\n",
        "\n",
        "The dataset we are going to work with today consists of 10,000 surnames, labelled with 18 different nationalities. Our first tasks will be to learn a classifier that can accurately assign a label to previously unseen surnames (note: the task is inherently very noisy). Later we will look at generating surnames. The dataset is split into 70% training data (from which we learn our network weights), 15% validation (which we use to evaluate and tune our network weights) and 15% test (which we use to quantitatively evaluate performance).\n",
        "\n",
        "A critical difference between last week's task (assigning positive or negative sentiment) and this weeks is that instead of two output classes, we now have 18. The only change we need to make is to the function applied to the output layer. Whereas before we had a single node in the output layer where we added up the weights and applied the sigmoid function to generate a probability of a positive label (and then we just subtract this value from 1 to give the probability of a negative label), now we have as many nodes as we have outcome labels and we generate a probability for each by applying the \"softmax\" function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-HbCEVc3JoA"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/gdrive\")\n",
        "#!mkdir /content/gdrive/My\\ Drive/CL_Week_5_Materials/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1U85sASKSul"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/surnames_with_splits.csv\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/mlp_tools.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/nn_tools_gen.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/generation_tools.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/rnn_tools.py\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/slp_model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/slp_vectorizer.json\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/mlp_model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/mlp_vectorizer.json\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/rnn_model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/rnn_vectorizer.json\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/gru_model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/gru_vectorizer.json\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/ugen_model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/ugen_vectorizer.json\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/cgen_model.pth\n",
        "!wget https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/CL_Week_5_Materials/cgen_vectorizer.json\n",
        "\n",
        "#!cp *py /content/gdrive/MyDrive/CL_Week_5_Materials/\n",
        "#!cp *json /content/gdrive/MyDrive/CL_Week_5_Materials/\n",
        "#!cp *csv /content/gdrive/MyDrive/CL_Week_5_Materials/\n",
        "#!cp *pth /content/gdrive/MyDrive/CL_Week_5_Materials/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpnWOxajoiYY"
      },
      "source": [
        "### Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqFmkcr0oiYY"
      },
      "outputs": [],
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "from nn_tools_gen import *\n",
        "from mlp_tools import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmQ7A-vR9MWH"
      },
      "source": [
        "### Multiclass Single layer neural network\n",
        "\n",
        "First we will use a single layer neural network. This is identical to the network that we used last week for sentiment classification, except for the output layer (that now applies a softmax function rather than a sigmoid function, as we have moved from 2 to >2 output classes).\n",
        "\n",
        "Instead of writing all the classification code we are going to use a tool called PyTorch. The point of including this is to examine the output - you do not need to understand the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbck87LC-AKV"
      },
      "outputs": [],
      "source": [
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A 1-layer Multilayer Perceptron for classifying surnames \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        prediction_vector = self.fc1(x_in)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "\n",
        "        return prediction_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXtobfIs-OJc"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"slp_vectorizer.json\",\n",
        "    model_state_file=\"slp_model.pth\",\n",
        "    #save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    save_dir=\".\",\n",
        "    # Model hyper parameters\n",
        "    hidden_dim=300,\n",
        "    # Training  hyper parameters\n",
        "    seed=1337,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=64,\n",
        "    # Runtime options\n",
        "    cuda=False,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_Az7-X7-Swz"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    print(\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    print(\"Creating fresh!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab),\n",
        "                               hidden_dim=args.hidden_dim,\n",
        "                               output_dim=len(vectorizer.nationality_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kgcqnl2L-hQu"
      },
      "outputs": [],
      "source": [
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_surname'])\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    print(str(batch_index) + \" \" + str(running_loss) + \" \" + str(running_acc))\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPGJ_-de-ms_"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOlQCx54-qvO"
      },
      "outputs": [],
      "source": [
        "new_surname = input(\"Enter a surname to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
        "                                    prediction['nationality'],\n",
        "                                    prediction['probability']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFLoMQ_FF6wm"
      },
      "source": [
        "If you want to test a few at a time you can put them in a list like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxYgBV8hF5y5"
      },
      "outputs": [],
      "source": [
        "# surname = input(\"Enter a surname: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
        "    print(predict_nationality(surname, classifier, vectorizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcJu2x9Lq3O2"
      },
      "source": [
        "### MultiLayer Perceptron\n",
        "\n",
        "Next we will use a multilayer perceptron. This is identical to the model above except that we add a hidden layer between the input and the output layers that combined the weighted input from the different first layers (input) nodes and learns a new set of weights to apply to the resulting combined values before submitting these new values to the output (softmax) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR5JvoX2q2VU"
      },
      "outputs": [],
      "source": [
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A 2-layer Multilayer Perceptron for classifying surnames \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_dim (int): the size of the input vectors\n",
        "            hidden_dim (int): the output size of the first Linear layer\n",
        "            output_dim (int): the output size of the second Linear layer\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        intermediate_vector = F.relu(self.fc1(x_in))\n",
        "        prediction_vector = self.fc2(intermediate_vector)\n",
        "\n",
        "        if apply_softmax:\n",
        "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
        "\n",
        "        return prediction_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2m7HbRmrLwy"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"mlp_vectorizer.json\",\n",
        "    model_state_file=\"mlp_model.pth\",\n",
        "    #save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    save_dir=\".\",\n",
        "    # Model hyper parameters\n",
        "    hidden_dim=300,\n",
        "    # Training  hyper parameters\n",
        "    seed=1337,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=64,\n",
        "    # Runtime options\n",
        "    cuda=False,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr6xbVLLrRU5"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    print(\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    print(\"Creating fresh!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "classifier = SurnameClassifier(input_dim=len(vectorizer.surname_vocab),\n",
        "                               hidden_dim=args.hidden_dim,\n",
        "                               output_dim=len(vectorizer.nationality_vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DygZuZENr1ke"
      },
      "outputs": [],
      "source": [
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_surname'])\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_nationality'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_nationality'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    print(str(batch_index) + \" \" + str(running_loss) + \" \" + str(running_acc))\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh51rENgr4YH"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDw4dbe3r8Gy"
      },
      "outputs": [],
      "source": [
        "new_surname = input(\"Enter a surname to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
        "                                    prediction['nationality'],\n",
        "                                    prediction['probability']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHh73FhnFzF-"
      },
      "source": [
        "If you want to test a few at a time you can put them in a list like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVM2JCMQFxnW"
      },
      "outputs": [],
      "source": [
        "# surname = input(\"Enter a surname: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
        "    print(predict_nationality(surname, classifier, vectorizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "LQ6W6LceGi3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf9H9Au5uvsV"
      },
      "source": [
        "### Recurrent Neural Network based classifier\n",
        "\n",
        "Next we use a simple recurrent neural network to transform the sequence input before then passing the resulting representation to a multilayer perceptron (see lecture). This is called an Elman network after Jeff Elman, who first applied such a network to language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjdr0Rcwu2Xj"
      },
      "outputs": [],
      "source": [
        "from rnn_tools import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUo1OeNBvH8V"
      },
      "outputs": [],
      "source": [
        "class ElmanRNN(nn.Module):\n",
        "    \"\"\" an Elman RNN built using the RNNCell \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): size of the input vectors\n",
        "            hidden_size (int): size of the hidden state vectors\n",
        "            bathc_first (bool): whether the 0th dimension is batch\n",
        "        \"\"\"\n",
        "        super(ElmanRNN, self).__init__()\n",
        "\n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def _initial_hidden(self, batch_size):\n",
        "        return torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "    def forward(self, x_in, initial_hidden=None):\n",
        "        \"\"\"The forward pass of the ElmanRNN\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\n",
        "                Else: x_in.shape = (seq_size, batch, feat_size)\n",
        "            initial_hidden (torch.Tensor): the initial hidden state for the RNN\n",
        "        Returns:\n",
        "            hiddens (torch.Tensor): The outputs of the RNN at each time step.\n",
        "                If self.batch_first: hiddens.shape = (batch, seq_size, hidden_size)\n",
        "                Else: hiddens.shape = (seq_size, batch, hidden_size)\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            batch_size, seq_size, feat_size = x_in.size()\n",
        "            x_in = x_in.permute(1, 0, 2)\n",
        "        else:\n",
        "            seq_size, batch_size, feat_size = x_in.size()\n",
        "\n",
        "        hiddens = []\n",
        "\n",
        "        if initial_hidden is None:\n",
        "            initial_hidden = self._initial_hidden(batch_size)\n",
        "            initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "        hidden_t = initial_hidden\n",
        "\n",
        "        for t in range(seq_size):\n",
        "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
        "            hiddens.append(hidden_t)\n",
        "\n",
        "        hiddens = torch.stack(hiddens)\n",
        "\n",
        "        if self.batch_first:\n",
        "            hiddens = hiddens.permute(1, 0, 2)\n",
        "\n",
        "        return hiddens\n",
        "\n",
        "\n",
        "\n",
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A Classifier with an RNN to extract features and an MLP to classify \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): The size of the character embeddings\n",
        "            num_embeddings (int): The number of characters to embed\n",
        "            num_classes (int): The size of the prediction vector\n",
        "                Note: the number of nationalities\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will\n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding;\n",
        "                see torch.nn.Embedding\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                embedding_dim=embedding_size,\n",
        "                                padding_idx=padding_idx)\n",
        "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
        "                             hidden_size=rnn_hidden_size,\n",
        "                             batch_first=batch_first)\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                         out_features=rnn_hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                          out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            x_lengths (torch.Tensor): the lengths of each sequence in the batch.\n",
        "                They are used to find the final vector of each sequence\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        x_embedded = self.emb(x_in)\n",
        "        y_out = self.rnn(x_embedded)\n",
        "\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjwGxtGnvSZd"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"rnn_vectorizer.json\",\n",
        "    model_state_file=\"rnn_model.pth\",\n",
        "    #save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    save_dir=\".\",\n",
        "    # Model hyper parameter\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # Training hyper parameter\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    seed=1337,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime hyper parameter\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5hnZSW9vdVa"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = SurnameClassifier(embedding_size=args.char_embedding_size,\n",
        "                               num_embeddings=len(vectorizer.char_vocab),\n",
        "                               num_classes=len(vectorizer.nationality_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvrPaE9qvjru"
      },
      "outputs": [],
      "source": [
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_data'],\n",
        "                         x_lengths=batch_dict['x_length'])\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    print(str(batch_index) + \" \" + str(running_loss) + \" \" + str(running_acc))\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oLhhhjIv35V"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cv_yNX4xe8D"
      },
      "outputs": [],
      "source": [
        "new_surname = input(\"Enter a surname to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_nationality_rnn(new_surname, classifier, vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
        "                                    prediction['nationality'],\n",
        "                                    prediction['probability']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4pdVerDFnph"
      },
      "source": [
        "If you want to test a few at a time you can put them in a list like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRWHi4DVv_j2"
      },
      "outputs": [],
      "source": [
        "# surname = input(\"Enter a surname: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
        "    print(predict_nationality_rnn(surname, classifier, vectorizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXkiVE3bE0Vr"
      },
      "source": [
        "### Gated Recurrent Unit (GRU)-based classifier\n",
        "\n",
        "We next take a similar approach but instead of a simple RNN we make use of a recurrent neural network with gating. As explained in the lecture this uses gates at each step (of the input, e.g. words or characters) in order to learn what information to pass along through the recurrent connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iwnFO-PE-8x"
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    \"\"\" a GRU network built using the GRU Cell \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): size of the input vectors\n",
        "            hidden_size (int): size of the hidden state vectors\n",
        "            bathc_first (bool): whether the 0th dimension is batch\n",
        "        \"\"\"\n",
        "        super(GRU, self).__init__()\n",
        "\n",
        "        self.rnn_cell = nn.GRUCell(input_size, hidden_size)\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def _initial_hidden(self, batch_size):\n",
        "        return torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "    def forward(self, x_in, initial_hidden=None):\n",
        "        \"\"\"The forward pass of the GRU\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\n",
        "                Else: x_in.shape = (seq_size, batch, feat_size)\n",
        "            initial_hidden (torch.Tensor): the initial hidden state for the RNN\n",
        "        Returns:\n",
        "            hiddens (torch.Tensor): The outputs of the RNN at each time step.\n",
        "                If self.batch_first: hiddens.shape = (batch, seq_size, hidden_size)\n",
        "                Else: hiddens.shape = (seq_size, batch, hidden_size)\n",
        "        \"\"\"\n",
        "        if self.batch_first:\n",
        "            batch_size, seq_size, feat_size = x_in.size()\n",
        "            x_in = x_in.permute(1, 0, 2)\n",
        "        else:\n",
        "            seq_size, batch_size, feat_size = x_in.size()\n",
        "\n",
        "        hiddens = []\n",
        "\n",
        "        if initial_hidden is None:\n",
        "            initial_hidden = self._initial_hidden(batch_size)\n",
        "            initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "        hidden_t = initial_hidden\n",
        "\n",
        "        for t in range(seq_size):\n",
        "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
        "            hiddens.append(hidden_t)\n",
        "\n",
        "        hiddens = torch.stack(hiddens)\n",
        "\n",
        "        if self.batch_first:\n",
        "            hiddens = hiddens.permute(1, 0, 2)\n",
        "\n",
        "        return hiddens\n",
        "\n",
        "\n",
        "\n",
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A Classifier with an RNN to extract features and an MLP to classify \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): The size of the character embeddings\n",
        "            num_embeddings (int): The number of characters to embed\n",
        "            num_classes (int): The size of the prediction vector\n",
        "                Note: the number of nationalities\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will\n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding;\n",
        "                see torch.nn.Embedding\n",
        "        \"\"\"\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,\n",
        "                                embedding_dim=embedding_size,\n",
        "                                padding_idx=padding_idx)\n",
        "        self.rnn = GRU(input_size=embedding_size,\n",
        "                             hidden_size=rnn_hidden_size,\n",
        "                             batch_first=batch_first)\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                         out_features=rnn_hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                          out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            x_lengths (torch.Tensor): the lengths of each sequence in the batch.\n",
        "                They are used to find the final vector of each sequence\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        x_embedded = self.emb(x_in)\n",
        "        y_out = self.rnn(x_embedded)\n",
        "\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HQrO220E-_v"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"gru_vectorizer.json\",\n",
        "    model_state_file=\"gru_model.pth\",\n",
        "    #save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    save_dir=\".\",\n",
        "    # Model hyper parameter\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # Training hyper parameter\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    seed=1337,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime hyper parameter\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_NPh5FcHBkj"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = SurnameClassifier(embedding_size=args.char_embedding_size,\n",
        "                               num_embeddings=len(vectorizer.char_vocab),\n",
        "                               num_classes=len(vectorizer.nationality_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW1sauBvHIgN"
      },
      "outputs": [],
      "source": [
        "train_state = make_train_state(args)\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset,\n",
        "                                   batch_size=args.batch_size,\n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_data'],\n",
        "                         x_lengths=batch_dict['x_length'])\n",
        "\n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    print(str(batch_index) + \" \" + str(running_loss) + \" \" + str(running_acc))\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RupB04stHJgT"
      },
      "outputs": [],
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KUhiUdGHUQ5"
      },
      "outputs": [],
      "source": [
        "new_surname = input(\"Enter a surname to classify: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "prediction = predict_nationality_rnn(new_surname, classifier, vectorizer)\n",
        "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
        "                                    prediction['nationality'],\n",
        "                                    prediction['probability']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjH9mD5rHnhe"
      },
      "source": [
        "### Unconditioned Generation with Recurrent Neural Networks\n",
        "\n",
        "This actually works very similarly in that it randomly picks each element of the sequence we are generating based on its probability in context. The difference is that the probabilities are based on the softmax output layer in a neural network rather than bigrams.\n",
        "\n",
        "We are going to randomly generate surnames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faDP0p4bHs3B"
      },
      "outputs": [],
      "source": [
        "from generation_tools import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTrTpUZnHxLU"
      },
      "outputs": [],
      "source": [
        "class SurnameGenerationModel(nn.Module):\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size,\n",
        "                 batch_first=True, padding_idx=0, dropout_p=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            char_embedding_size (int): The size of the character embeddings\n",
        "            char_vocab_size (int): The number of characters to embed\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will\n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding;\n",
        "                see torch.nn.Embedding\n",
        "            dropout_p (float): the probability of zeroing activations using\n",
        "                the dropout method.  higher means more likely to zero.\n",
        "        \"\"\"\n",
        "        super(SurnameGenerationModel, self).__init__()\n",
        "\n",
        "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
        "                                     embedding_dim=char_embedding_size,\n",
        "                                     padding_idx=padding_idx)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=char_embedding_size,\n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=batch_first)\n",
        "\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
        "                            out_features=char_vocab_size)\n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "    def forward(self, x_in, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the model\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\n",
        "        \"\"\"\n",
        "        x_embedded = self.char_emb(x_in)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded)\n",
        "\n",
        "        batch_size, seq_size, feat_size = y_out.shape\n",
        "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        new_feat_size = y_out.shape[-1]\n",
        "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
        "\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7andkb_H3yE"
      },
      "outputs": [],
      "source": [
        "def sample_from_model(model, vectorizer, num_samples=1, sample_size=20,\n",
        "                      temperature=1.0):\n",
        "    \"\"\"Sample a sequence of indices from the model\n",
        "\n",
        "    Args:\n",
        "        model (SurnameGenerationModel): the trained model\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
        "        num_samples (int): the number of samples\n",
        "        sample_size (int): the max length of the samples\n",
        "        temperature (float): accentuates or flattens\n",
        "            the distribution.\n",
        "            0.0 < temperature < 1.0 will make it peakier.\n",
        "            temperature > 1.0 will make it more uniform\n",
        "    Returns:\n",
        "        indices (torch.Tensor): the matrix of indices;\n",
        "        shape = (num_samples, sample_size)\n",
        "    \"\"\"\n",
        "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index\n",
        "                       for _ in range(num_samples)]\n",
        "    begin_seq_index = torch.tensor(begin_seq_index,\n",
        "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
        "    indices = [begin_seq_index]\n",
        "    h_t = None\n",
        "\n",
        "    for time_step in range(sample_size):\n",
        "        x_t = indices[time_step]\n",
        "        x_emb_t = model.char_emb(x_t)\n",
        "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
        "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
        "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
        "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
        "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
        "    return indices\n",
        "\n",
        "def decode_samples(sampled_indices, vectorizer):\n",
        "    \"\"\"Transform indices into the string form of a surname\n",
        "\n",
        "    Args:\n",
        "        sampled_indices (torch.Tensor): the inidces from `sample_from_model`\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
        "    \"\"\"\n",
        "    decoded_surnames = []\n",
        "    vocab = vectorizer.char_vocab\n",
        "\n",
        "    for sample_index in range(sampled_indices.shape[0]):\n",
        "        surname = \"\"\n",
        "        for time_step in range(sampled_indices.shape[1]):\n",
        "            sample_item = sampled_indices[sample_index, time_step].item()\n",
        "            if sample_item == vocab.begin_seq_index:\n",
        "                continue\n",
        "            elif sample_item == vocab.end_seq_index:\n",
        "                break\n",
        "            else:\n",
        "                surname += vocab.lookup_index(sample_item)\n",
        "        decoded_surnames.append(surname)\n",
        "    return decoded_surnames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cirv00CmIFcp"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"ugen_vectorizer.json\",\n",
        "    model_state_file=\"ugen_model.pth\",\n",
        "    #save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    save_dir=\".\",\n",
        "    # Model hyper parameters\n",
        "    char_embedding_size=32,\n",
        "    rnn_hidden_size=32,\n",
        "    # Training hyper parameters\n",
        "    seed=1337,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txMECyJUIGVq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
        "                               char_vocab_size=len(vectorizer.char_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "model.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsPcEUe5IKfI"
      },
      "outputs": [],
      "source": [
        "# number of names to generate\n",
        "num_names = 10\n",
        "model = model.cpu()\n",
        "# Generate nationality hidden state\n",
        "sampled_surnames = decode_samples(\n",
        "    sample_from_model(model, vectorizer, num_samples=num_names),\n",
        "    vectorizer)\n",
        "# Show results\n",
        "print (\"-\"*15)\n",
        "for i in range(num_names):\n",
        "    print (sampled_surnames[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8zGHzgRPxXS"
      },
      "source": [
        "### Conditioned Generation\n",
        "\n",
        "We now repeat this, but we condition the output of the network on something that isn't in the sequence - a target nationality for the surname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNNm6G-tP0jq"
      },
      "outputs": [],
      "source": [
        "class SurnameGenerationModel(nn.Module):\n",
        "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0, dropout_p=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            char_embedding_size (int): The size of the character embeddings\n",
        "            char_vocab_size (int): The number of characters to embed\n",
        "            num_nationalities (int): The size of the prediction vector\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will\n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding;\n",
        "                see torch.nn.Embedding\n",
        "            dropout_p (float): the probability of zeroing activations using\n",
        "                the dropout method.  higher means more likely to zero.\n",
        "        \"\"\"\n",
        "        super(SurnameGenerationModel, self).__init__()\n",
        "\n",
        "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
        "                                     embedding_dim=char_embedding_size,\n",
        "                                     padding_idx=padding_idx)\n",
        "\n",
        "        self.nation_emb = nn.Embedding(num_embeddings=num_nationalities,\n",
        "                                       embedding_dim=rnn_hidden_size)\n",
        "\n",
        "        self.rnn = nn.GRU(input_size=char_embedding_size,\n",
        "                          hidden_size=rnn_hidden_size,\n",
        "                          batch_first=batch_first)\n",
        "\n",
        "        self.fc = nn.Linear(in_features=rnn_hidden_size,\n",
        "                            out_features=char_vocab_size)\n",
        "\n",
        "        self._dropout_p = dropout_p\n",
        "\n",
        "    def forward(self, x_in, nationality_index, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the model\n",
        "\n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor.\n",
        "                x_in.shape should be (batch, max_seq_size)\n",
        "            nationality_index (torch.Tensor): The index of the nationality for each data point\n",
        "                Used to initialize the hidden state of the RNN\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\n",
        "        \"\"\"\n",
        "        x_embedded = self.char_emb(x_in)\n",
        "\n",
        "        # hidden_size: (num_layers * num_directions, batch_size, rnn_hidden_size)\n",
        "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\n",
        "\n",
        "        y_out, _ = self.rnn(x_embedded, nationality_embedded)\n",
        "\n",
        "        batch_size, seq_size, feat_size = y_out.shape\n",
        "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
        "\n",
        "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "\n",
        "        new_feat_size = y_out.shape[-1]\n",
        "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
        "\n",
        "        return y_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRxBaKnoQc6x"
      },
      "outputs": [],
      "source": [
        "def sample_from_model(model, vectorizer, nationalities, sample_size=20,\n",
        "                      temperature=1.0):\n",
        "    \"\"\"Sample a sequence of indices from the model\n",
        "\n",
        "    Args:\n",
        "        model (SurnameGenerationModel): the trained model\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
        "        nationalities (list): a list of integers representing nationalities\n",
        "        sample_size (int): the max length of the samples\n",
        "        temperature (float): accentuates or flattens\n",
        "            the distribution.\n",
        "            0.0 < temperature < 1.0 will make it peakier.\n",
        "            temperature > 1.0 will make it more uniform\n",
        "    Returns:\n",
        "        indices (torch.Tensor): the matrix of indices;\n",
        "        shape = (num_samples, sample_size)\n",
        "    \"\"\"\n",
        "    num_samples = len(nationalities)\n",
        "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index\n",
        "                       for _ in range(num_samples)]\n",
        "    begin_seq_index = torch.tensor(begin_seq_index,\n",
        "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
        "    indices = [begin_seq_index]\n",
        "    nationality_indices = torch.tensor(nationalities, dtype=torch.int64).unsqueeze(dim=0)\n",
        "    h_t = model.nation_emb(nationality_indices)\n",
        "\n",
        "    for time_step in range(sample_size):\n",
        "        x_t = indices[time_step]\n",
        "        x_emb_t = model.char_emb(x_t)\n",
        "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
        "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
        "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
        "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
        "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
        "    return indices\n",
        "\n",
        "def decode_samples(sampled_indices, vectorizer):\n",
        "    \"\"\"Transform indices into the string form of a surname\n",
        "\n",
        "    Args:\n",
        "        sampled_indices (torch.Tensor): the inidces from `sample_from_model`\n",
        "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
        "    \"\"\"\n",
        "    decoded_surnames = []\n",
        "    vocab = vectorizer.char_vocab\n",
        "\n",
        "    for sample_index in range(sampled_indices.shape[0]):\n",
        "        surname = \"\"\n",
        "        for time_step in range(sampled_indices.shape[1]):\n",
        "            sample_item = sampled_indices[sample_index, time_step].item()\n",
        "            if sample_item == vocab.begin_seq_index:\n",
        "                continue\n",
        "            elif sample_item == vocab.end_seq_index:\n",
        "                break\n",
        "            else:\n",
        "                surname += vocab.lookup_index(sample_item)\n",
        "        decoded_surnames.append(surname)\n",
        "    return decoded_surnames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhMRf2byTDdY"
      },
      "outputs": [],
      "source": [
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    surname_csv=\"surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"cgen_vectorizer.json\",\n",
        "    model_state_file=\"cgen_model.pth\",\n",
        "    #save_dir=\"/content/gdrive/My Drive/CL_Week_5_Materials/\",\n",
        "    save_dir=\".\",\n",
        "    # Model hyper parameters\n",
        "    char_embedding_size=32,\n",
        "    rnn_hidden_size=32,\n",
        "    # Training hyper parameters\n",
        "    seed=1337,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=128,\n",
        "    num_epochs=100,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir,\n",
        "                                        args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir,\n",
        "                                         args.model_state_file)\n",
        "\n",
        "    print(\"Expanded filepaths: \")\n",
        "    print(\"\\t{}\".format(args.vectorizer_file))\n",
        "    print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-wcWeg0S8A3"
      },
      "outputs": [],
      "source": [
        "if args.reload_from_files:\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
        "                                                              args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "model = SurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
        "                               char_vocab_size=len(vectorizer.char_vocab),\n",
        "                               num_nationalities=len(vectorizer.nationality_vocab),\n",
        "                               rnn_hidden_size=args.rnn_hidden_size,\n",
        "                               padding_idx=vectorizer.char_vocab.mask_index,\n",
        "                               dropout_p=0.5)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "model.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "model = model.to(args.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjMXT0FITVN7"
      },
      "outputs": [],
      "source": [
        "model = model.cpu()\n",
        "for index in range(len(vectorizer.nationality_vocab)):\n",
        "    nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "    print(\"Sampled for {}: \".format(nationality))\n",
        "    sampled_indices = sample_from_model(model, vectorizer,\n",
        "                                        nationalities=[index] * 3,\n",
        "                                        temperature=0.7)\n",
        "    for sampled_surname in decode_samples(sampled_indices, vectorizer):\n",
        "        print(\"-  \" + sampled_surname)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "156px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": "5",
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}