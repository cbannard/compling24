{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RByeKCfdaSZ7"
      },
      "source": [
        "# LELA32052 Computational Linguistics Week 5\n",
        "\n",
        "This week we are going to start to look at machine learning - specifically at supervised machine learning. Supervised machine learning refers to any situation where the computer is given a set of inputs and outputs, and learns a model that allows it to turn new inputs into outputs. Our main focus today will be on classification - where a set of words or sentences are given as input, and the machine must learn to assign one of a set of finite labels as output.\n",
        "\n",
        "However, we will build up to this and will start with an even simpler form of machine learning  - linear regression.\n",
        "\n",
        "In linear regression the machine learns to map input to continuous valued outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following data we have continuous values as input (X, on the horizontal axis) and output (Y, on the vertical axis)"
      ],
      "metadata": {
        "id": "F0iBE1BhBavF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "x = [1.00,1.25,1.50,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,3.75,4.00]\n",
        "y = [33,49,41,54,52,45,36,58,45,69,55,56,68]\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.ylim(0,100)\n",
        "plt.xlim(0,5)\n",
        "plt.scatter(x, y)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nPCQWoBdAZ4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume that is it possible to predict y using a linear equation - consisting of just a bias term (known in statistics as the intercept) and slope. This can be visualised as a straight line.\n",
        "\n",
        "Y = Bias + X*Slope"
      ],
      "metadata": {
        "id": "Vn9n-Z0pB0a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line_x = [0, 5]\n",
        "line_y = [6, 6+75]\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(line_x, line_y, label='Line', color='red')  # Adding a line"
      ],
      "metadata": {
        "id": "0j7V4RqHdwzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The machine learns to do this by finding the values of the bias and the slope that minimises the \"loss\".\n",
        "\n",
        "The loss is \"the mean squared error\" - we calculate the difference of each predicted value (as described by the line) and the target value (the dots), we square each value, and we take the mean."
      ],
      "metadata": {
        "id": "kC5bwmNpCUQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 30\n",
        "num_features=1\n",
        "weight = np.random.rand(num_features)[0]\n",
        "bias=0\n",
        "linear_loss=[]\n",
        "\n",
        "num_samples = len(y)\n",
        "lr=0.01\n",
        "\n",
        "for i in range(n_iters):\n",
        "    y_est = x*weight+bias\n",
        "    #print(y_est)\n",
        "    #Â´print(y)\n",
        "    errors = y_est-y\n",
        "    #print(errors)\n",
        "    loss = errors.dot(errors)/num_samples\n",
        "    linear_loss.append(loss)\n",
        "\n",
        "    dw = (1 / num_samples) * sum(x*errors)\n",
        "    db = (1 / num_samples) * sum(errors)\n",
        "    weight = weight - lr * dw\n",
        "    bias = bias - lr * db\n",
        "    line_x = [0, 5]\n",
        "    line_y = [bias, bias+(5*weight)]\n",
        "\n",
        "    plt.scatter(x, y)\n",
        "    plt.plot(line_x, line_y, label='Line', color='red')  # Adding a line\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plt.plot(range(1,n_iters),linear_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "EfE0cfR4BXcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias"
      ],
      "metadata": {
        "id": "tK8W88xrd1VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight"
      ],
      "metadata": {
        "id": "pO_i-qVnd3HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1: We can think of X as the number of days a students studied for an exam, and Y as the mark they received. Calculate the expected exam grade for students who studied for the following amounts of time:\n",
        "a) 0 days\n",
        "b) 3 days\n",
        "c) 4.5 days"
      ],
      "metadata": {
        "id": "tlLuCqZSIR9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Next we are going to look at  1-layer neural networks, also known as logistic regression models or as perceptrons. These were introduced to you in abstract in the lecture and in this seminar we are going to look at how they work in reality.\n",
        "\n",
        "Perceptrons are commonly used as binary classifiers - applying one of two possible labels to input. The example that we are going to look at today is sentiment classification, where we classify a text as having either a \"negative\" or \"positive\" perspective on whatever it is discussing, e.g. a product it is reviewing.\n",
        "\n",
        "First of all we will look at a toy problem with some made up data, then we will look at some real sentiment data. The stars in the plot are the reviews with positive sentiment. The circles are reviews with negative sentiment."
      ],
      "metadata": {
        "id": "2-athMgccinO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Create simulated data\n",
        "np.random.seed(10)\n",
        "w1_center = (2, 3)\n",
        "w2_center = (3, 2)\n",
        "batch_size=50\n",
        "\n",
        "x = np.zeros((batch_size, 2))\n",
        "y = np.zeros(batch_size)\n",
        "for i in range(batch_size):\n",
        "    if np.random.random() > 0.5:\n",
        "        x[i] = np.random.normal(loc=w1_center)\n",
        "    else:\n",
        "        x[i] = np.random.normal(loc=w2_center)\n",
        "        y[i] = 1\n",
        "\n",
        "x=x.T\n",
        "\n",
        "plt.scatter(x[0][y==0], x[1][y==0], marker='*', s=100)\n",
        "plt.scatter(x[0][y==1], x[1][y==1], marker='o', s=100)\n",
        "plt.xlabel(\"log count of negative words\")\n",
        "plt.ylabel(\"log count of positive words\")\n",
        "plt.xlim((0,5))\n",
        "plt.ylim((0,5))"
      ],
      "metadata": {
        "id": "U97O2H3ZClIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now instead of just one input value X we have two input values x_1 and x_2.\n",
        "The learning component works similarly to the linear regression case, except that instead of a bias and a single slope, we have a bias and two \"weights\".\n",
        "\n",
        "Z = Bias + x_1*Weight1 + x_2 *Weight2"
      ],
      "metadata": {
        "id": "5VCFKpbxEd3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "num_features=2\n",
        "weights = np.random.rand(num_features)\n",
        "bias=0\n",
        "\n",
        "n_iters = 100\n",
        "num_features = 2\n",
        "num_samples = len(y)\n",
        "lr=0.01\n",
        "logistic_loss=[]\n",
        "\n",
        "xmin, xmax = 0, 5\n",
        "ymin, ymax = 0, 5\n",
        "xd = np.array([xmin, xmax])\n",
        "\n",
        "for i in range(n_iters):\n",
        "    #z=x[0]*weights[0] + x[1]*weights[1] + bias\n",
        "    z=weights.dot(x) + bias\n",
        "    q = 1/(1+np.exp(-z))\n",
        "    loss = sum(-(y*np.log2(q)+(1-y)*np.log2(1-q)))/num_samples\n",
        "    logistic_loss.append(loss)\n",
        "    dw1 = sum(x[0]*(q-y))/num_samples\n",
        "    dw2 = sum(x[1]*(q-y))/num_samples\n",
        "    db = sum(q-y)/num_samples\n",
        "    weights[0] = weights[0] - dw1*lr\n",
        "    weights[1] = weights[1] - dw2*lr\n",
        "    bias = bias - db*lr\n",
        "    plt.scatter(x[0][y==0], x[1][y==0], marker='*', s=100)\n",
        "    plt.scatter(x[0][y==1], x[1][y==1], marker='o', s=100)\n",
        "    plt.xlim((0,5))\n",
        "    plt.ylim((0,5))\n",
        "    c = -bias/weights[1]\n",
        "    m = -weights[0]/weights[1]\n",
        "    yd = m*xd + c\n",
        "    plt.plot(xd, yd, 'k', lw=1, ls='--')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")"
      ],
      "metadata": {
        "id": "xcP1ZkY3Cxo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights"
      ],
      "metadata": {
        "id": "ZlgSKD1eF6MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bias"
      ],
      "metadata": {
        "id": "o7CLZw0SF95k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2: Given the bias and weights above Calculate Z for the following inputs\n",
        "\n",
        "a) X1 of 2 and X2 of 4\n",
        "\n",
        "b) X1 of 3 and X2 of 1"
      ],
      "metadata": {
        "id": "1j5-Y502F_aj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want our output to be a probability. We therefore convert Z into a value between 0 and 1 using the sigmoid function (See Lecture)"
      ],
      "metadata": {
        "id": "qHgEXvFRGndk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return(1/(1+np.exp(-z)))"
      ],
      "metadata": {
        "id": "PsZLrTBsGzvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sigmoid(1.5)"
      ],
      "metadata": {
        "id": "zg__-oIXHMF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3. Convert the Z values from Problem 2 into probabilities. These values are p(Positive Sentiment | X)"
      ],
      "metadata": {
        "id": "evq47IBnHuPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression with text: sentiment analysis\n",
        "\n",
        "The dataset we are going to use here is 10000 reviews on Yelp classified as negative (1 or 2 star) or positive (3 or 4 star). We are going to train a classifier using a part of this data and test its performance on another part.\n",
        "\n",
        "First we download and load the dataset:"
      ],
      "metadata": {
        "id": "OC5U-WIK5AzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/cbannard/lela60331_24-25/refs/heads/main/data/yelp_reviews.txt\n",
        "\n",
        "# Create lists\n",
        "reviews=[]\n",
        "labels=[]\n",
        "\n",
        "with open(\"yelp_reviews.txt\") as f:\n",
        "   # iterate over the lines in the file\n",
        "   for line in f.readlines()[1:]:\n",
        "        # split the current line into a list of two element - the review and the label\n",
        "        fields = line.rstrip().split('\\t')\n",
        "        # put the current review in the reviews list\n",
        "        reviews.append(fields[0])\n",
        "        # put the current sentiment rating in the labels list\n",
        "        labels.append(fields[1])"
      ],
      "metadata": {
        "id": "IBgjrmTybd7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[6000]"
      ],
      "metadata": {
        "id": "G3SnCiEWbk4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels[6000]"
      ],
      "metadata": {
        "id": "hlHYr55abmwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-hot coding\n",
        "\n",
        "We are going to represent our data using one-hot encoding. We need to use the same vocabulary for our training and test data so we do this prior to splitting the data.\n",
        "\n",
        "In order to one-hot encode we need to create a list of the included vocabulary items. We will use the 5000 most frequent words. To get this list we extract all the words from all the reviews, count how often they occur, sort them and then take the most frequent 5000 words."
      ],
      "metadata": {
        "id": "0hqHqxKGSJwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "# Tokenise the text, turning a list of strings into a list of lists of tokens. We use very naive space-based tokenisation.\n",
        "tokenized_sents = [re.findall(\"[^ ]+\",txt) for txt in reviews]\n",
        "# Collapse all tokens into a single list\n",
        "tokens=[]\n",
        "for s in tokenized_sents:\n",
        "      tokens.extend(s)\n",
        "# Count the tokens in the tokens list. The returns a list of tuples of each token and count\n",
        "counts=Counter(tokens)\n",
        "# Sort the tuples. The reverse argument instructs to put most frequent first rather than last (which is the default)\n",
        "so=sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
        "# Extract the list of tokens, by transposing the list of lists so that there is a list of tokens a list of counts and then just selecting the former\n",
        "so=list(zip(*so))[0]\n",
        "# Select the firs 5000 words in the list\n",
        "type_list=so[0:5000]"
      ],
      "metadata": {
        "id": "rXVFg3H442fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to one-hot encode our reviews. We have 10000 reviews and a selected vocabulary of 5000 words. We therefore want to end up with 10000 x 5000 matrix **M**, where each row $i$ is a review, each column $j$ is a unique word from the vocab, and each element $x_{i,j}$ is a one if the word j occurs in review i and a zero otherwise."
      ],
      "metadata": {
        "id": "E5SAuA6QaZXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 10000 x 5000 matrix of zeros\n",
        "M = np.zeros((len(reviews), len(type_list)))\n",
        "#iterate over the reviews\n",
        "for i, rev in enumerate(reviews):\n",
        "    # Tokenise the current review:\n",
        "    tokens = re.findall(\"[^ ]+\",rev)\n",
        "    # iterate over the words in our type list (the set of 5000 words):\n",
        "    for j,t in enumerate(type_list):\n",
        "        # if the current word j occurs in the current review i then set the matrix element at i,j to be one. Otherwise leave as zero.\n",
        "        if t in tokens:\n",
        "              M[i,j] = 1\n",
        "\n"
      ],
      "metadata": {
        "id": "5HgWE38SMIFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This one-hot vector:"
      ],
      "metadata": {
        "id": "oRxDGapPt9ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M[100,]"
      ],
      "metadata": {
        "id": "vZQgWwYGg_4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is a representation of this review"
      ],
      "metadata": {
        "id": "kdvfJ27VuAfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[100]"
      ],
      "metadata": {
        "id": "HuM34iSphDn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For which this is the label"
      ],
      "metadata": {
        "id": "qrRGD4ttuEOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels[100]"
      ],
      "metadata": {
        "id": "UTZi_zPdhMks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to split our data. We are going to use 20% of our data as test items, so we randomly select 8000 indices between 0 and 9999, which are the indices of our training items. The remaining 2000 indices are the indices of our test items.\n",
        "\n",
        "In a real development task we would want to split data into training, development and test. Here we just use training and test."
      ],
      "metadata": {
        "id": "ccEkUhsheHR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ints=np.random.choice(len(reviews),int(len(reviews)*0.8),replace=False)\n",
        "test_ints=list(set(range(0,len(reviews))) - set(train_ints))"
      ],
      "metadata": {
        "id": "JjTsjjDfCa5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We next use the indices to select the rows of our one-hot-encoded matrix M that correspond to our training items and our test items and put these into two separate matrices. We also select the corresponding labels."
      ],
      "metadata": {
        "id": "cDyfEsoRevxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M_train = M[train_ints,]\n",
        "M_test = M[test_ints,]\n",
        "labels_train = [labels[i] for i in train_ints]\n",
        "labels_test = [labels[i] for i in test_ints]"
      ],
      "metadata": {
        "id": "7GzcI2BeDWap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train our model using the training data."
      ],
      "metadata": {
        "id": "Keub330AfSJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "num_features=5000\n",
        "y=[int(l == \"positive\") for l in labels_train]\n",
        "weights = np.random.rand(num_features)\n",
        "bias=np.random.rand(1)\n",
        "n_iters = 500\n",
        "lr=0.4\n",
        "logistic_loss=[]\n",
        "num_samples=len(y)\n",
        "for i in range(n_iters):\n",
        "  z = M_train.dot(weights)+bias\n",
        "  q = 1/(1+np.exp(-z))\n",
        "  eps=0.00001\n",
        "  loss = -sum((y*np.log2(q+eps)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q+eps)))\n",
        "  logistic_loss.append(loss)\n",
        "  y_pred=[int(ql > 0.5) for ql in q]\n",
        "\n",
        "  dw = (q-y).dot(M_train)/num_samples\n",
        "  db = sum((q-y))/num_samples\n",
        "  weights = weights - lr*dw\n",
        "  bias = bias - lr*db\n",
        "\n",
        "plt.plot(range(1,n_iters),logistic_loss[1:])\n",
        "plt.xlabel(\"number of epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "#loss = sum(-(np.ones(len(y))*np.log2(q)+(np.ones(len(y))-y)*np.log2(np.ones(len(y))-q)))"
      ],
      "metadata": {
        "id": "My1xq84sUqUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a fitting model, we can use it to predict labels for our test items. The test reviews are in the one-hot matrix M_test. The labels for the test reviews are in the list labels_test.\n"
      ],
      "metadata": {
        "id": "-l-FYqk3f58c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = M_test.dot(weights)+bias\n",
        "q = 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "L9w01GfgfG8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test=[int(l == \"positive\") for l in labels_test]\n",
        "y_test_pred=[int(ql > 0.5) for ql in q]\n",
        "acc_test=[int(yp == y_test[s]) for s,yp in enumerate(y_test_pred)]\n",
        "print(sum(acc_test)/len(acc_test))"
      ],
      "metadata": {
        "id": "ZOz9DW4LI97R"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "156px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": "5",
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}