{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LELA32052 Computational Linguistics Week 4\n",
        "\n",
        "## N-gram language models\n",
        "\n",
        "In this seminar we will examine n-gram language models. The first thing that we will do is to build a table of bigram counts.\n",
        "\n",
        "In order to this I will need to introduce you to another data structure in Python - the dictionary.\n"
      ],
      "metadata": {
        "id": "TiYzKVynOejk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRhjvS71Mwk8"
      },
      "source": [
        "### Dictionaries\n",
        "In an earlier session you encountered the List. A second useful Python data structure is the Dictionary. This stores data in key and value pairs. There is a flexibility in the data types that can be keys and can be values, for example the former could be a string or an int. The latter could be a list or even another dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzZyc9SENAqp"
      },
      "outputs": [],
      "source": [
        "thisdict = {\n",
        "  \"brand\": \"Ford\",\n",
        "  \"model\": \"Mustang\",\n",
        "  \"year\": 1964\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkBZ6gCANMrL"
      },
      "outputs": [],
      "source": [
        "print(thisdict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpnWMsHuKxL5"
      },
      "source": [
        "You can obtain the keys as a standalone list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtrlqssuK3Il"
      },
      "outputs": [],
      "source": [
        "thisdict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoyoyS9qK68-"
      },
      "source": [
        "And the same for the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGy3_9SBK92h"
      },
      "outputs": [],
      "source": [
        "thisdict.values()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One useful additional thing to consider is that there are different kinds of dictionaries in the Collections library. We will make use of one special kind of dictionary later - the default dictionary which returns a default value when asked for a missing key."
      ],
      "metadata": {
        "id": "FeYbHC_pL3UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Loops\n",
        "\n",
        "A commonly used tool in programming is the \"for loop\". In its simplest form this allows us to iterate over (move through) a series of values. For example:"
      ],
      "metadata": {
        "id": "vhbvq9RwJpg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,10):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "et2uQKbmKMXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can also be used to iterate through data structures:\n"
      ],
      "metadata": {
        "id": "qYmClKR9K00Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=[\"the\",\"boy\",\"went\",\"to\",\"the\",\"park\"]\n",
        "for word in sentence:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "5lHU56r7K9mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LY_zLsR10_7"
      },
      "source": [
        "You can iterate over the keys and values of the dictionary that we created above as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcamhP911V--"
      },
      "outputs": [],
      "source": [
        "for key, value in thisdict.items():\n",
        "    print(key + \" \" + str(value))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram Counts\n",
        "\n",
        "We are going to extract bigram counts from a text by using a for loop to iterate over our text, and a dictionary (specifically a defaultdict) to store the counts"
      ],
      "metadata": {
        "id": "t8g4xwItLX_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we prepare the text by tokenising it into a list of words. We also add a sentence boundary character \"eol\""
      ],
      "metadata": {
        "id": "74S4TZeZyPwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# download from from the internet\n",
        "!wget https://www.gutenberg.org/files/2554/2554-0.txt\n",
        "# read in the file\n",
        "f = open('2554-0.txt')\n",
        "c_and_p = f.read()\n",
        "# Remove the title page etc\n",
        "# convert text to lower case\n",
        "c_and_p=c_and_p[5464:]\n",
        "c_and_p=c_and_p.lower()\n",
        "c_and_p=re.sub('\\n',' ', c_and_p)\n",
        "# Add end of sentence token\n",
        "c_and_p=re.sub(\"\\. \",\" eol \", c_and_p)\n",
        "c_and_p=re.sub('[^a-z ]','', c_and_p)\n",
        "c_and_p=re.sub(' +', ' ',c_and_p)\n",
        "c_and_p=re.split(\" \", c_and_p)\n"
      ],
      "metadata": {
        "id": "a0LXrQmvMFMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_and_p"
      ],
      "metadata": {
        "id": "FsmVSaLnMbQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we iterate over the text to extract bigram counts"
      ],
      "metadata": {
        "id": "FPrgR2ksMXRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "total_unigrams = len(c_and_p) - 1\n",
        "bigrams = defaultdict(int)\n",
        "unigrams = defaultdict(int)\n",
        "for i in range(total_unigrams-2):\n",
        "    unigrams[c_and_p[i]] += 1\n",
        "    bigrams[str.join(\" \",c_and_p[i:i+2])] += 1"
      ],
      "metadata": {
        "id": "XIZTxxscMWsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating the probabilities of sentences\n",
        "We can then use the chain rule with a Markov assumption in order to calculate the probability of sentences:\n",
        "\n",
        "# $p(the\\ dog\\ runs) = p( the| eol ) * p(dog|the) * p(runs|dog)$\n",
        "\n",
        "In order to deal with unseen bigrams we will use add-one smoothing"
      ],
      "metadata": {
        "id": "_MBqNcbzPET7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"the man came out\"\n",
        "sentence=sentence.split()\n",
        "sentence.insert(0,\"eol\")\n",
        "pr=1\n",
        "for i in range(len(sentence)-1):\n",
        "    ugr = sentence[i]\n",
        "    bgr = sentence[i] + \" \" + sentence[i+1]\n",
        "    pr *= (bigrams[bgr]+1)/(unigrams[ugr]+len(unigrams))\n",
        "format(pr, '.50f')"
      ],
      "metadata": {
        "id": "AOOlBlRQNAyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1a: See what the highest probability 5 word sentence you can come up with is."
      ],
      "metadata": {
        "id": "ckoaa2DWOZHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1b: See what the lowest probability 5 word sentence you can come up with is."
      ],
      "metadata": {
        "id": "-Rr090rENN_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation with language models\n",
        "\n",
        "Just as we can calculate the probability of a known string using the chain rule and Markov assumption, we can also incrementally generate sentences via a \"Markov process\". The probability of any sentence being generated can be calculated using chain rule.\n",
        "\n",
        "# $p(the\\ dog\\ runs) = p( the| eol ) * p(dog|the) * p(runs|dog)$"
      ],
      "metadata": {
        "id": "IzWExu1TOm90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![decoding](https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/images/decoding.png)"
      ],
      "metadata": {
        "id": "9Q5NDEz5QCjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Search\n",
        "\n",
        "In order to incrementally generate sentences the most obvious way to proceed is just to output the most probable word at each step.\n",
        "\n",
        "![greedy](https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/images/greedysearch.png)"
      ],
      "metadata": {
        "id": "H8T3Ia3vO2K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### While loops\n",
        "In generating sentences we are going to make use of a second very useful type of loop - the while loop. This allows us to repeatedly perform some operation while a particular statement is true. For example"
      ],
      "metadata": {
        "id": "phMIfJ5XyTua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "while i < 10:\n",
        "  i=i+1\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "nFdk9qpXOEfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start generating we are going to convert our bigrams counts into probabilities for convenience."
      ],
      "metadata": {
        "id": "R76MsasPSHP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nested_dict = lambda: defaultdict(nested_dict)\n",
        "d = nested_dict()\n",
        "\n",
        "for bg in bigrams:\n",
        "  ug = bg.split()\n",
        "  print(bg)\n",
        "  d[ug[1]][ug[0]] = bigrams[bg]/unigrams[ug[0]]\n",
        "\n",
        "lm=pd.DataFrame(d)\n",
        "lm"
      ],
      "metadata": {
        "id": "T1thBKZFSFzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try greedy decoding by running the code below with different starting words. What do you notice?"
      ],
      "metadata": {
        "id": "72sOgdKyTpqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define starting word\n",
        "w=\"i\"\n",
        "# Define stopping point - here when an end of line character is output\n",
        "while w != \"eol\":\n",
        "  print(w,end=' ')\n",
        "  # get probabilities for all words following the previous word\n",
        "  s=lm.loc[w]\n",
        "  # sort the probabilities and output the most likely word\n",
        "  w=s.sort_values(ascending=False).index[0]\n"
      ],
      "metadata": {
        "id": "Y52DF1CtOq4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "\n",
        "Another way to pick words to output is to randomly choose them, but weighted by their probability in context. The probability then is the proportion of runs for which that word will be chosen.\n",
        "\n",
        "![sampling](https://raw.githubusercontent.com/cbannard/compling24/refs/heads/main/images/sampling.png)"
      ],
      "metadata": {
        "id": "sX20xrCyO5ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Specify starting word\n",
        "w=\"he\"\n",
        "# Define stopping point - here when an end of line character is output\n",
        "while w != \"eol\":\n",
        "  print(w,end=' ')\n",
        "  # get probabilities for all words following the previous word\n",
        "  s=lm.loc[w]\n",
        "  s=s.drop(s[np.isnan(s)].index)\n",
        "  # Choose randomly from the probability distribution over next words\n",
        "  w=np.random.choice(list(s.index),1,list(np.exp(s.values)))[0]"
      ],
      "metadata": {
        "id": "obIaEmI6O-mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What problems do you notice with these sampled sentences?"
      ],
      "metadata": {
        "id": "JuYd3u98Na9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating with GPT\n",
        "\n",
        "The problems that you see with the generated output above isn't just a result of the simple bigram model used to generate the probabilities. The same problems apply even when using a more sophisticated language model such as GPT."
      ],
      "metadata": {
        "id": "1Q8dZyF1QT-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q tensorflow==2.1\n",
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "HBmX4TyPPii0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Search Generation with GPT-2"
      ],
      "metadata": {
        "id": "7pjOL59DQ3pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='tf')\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "greedy_output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "xXb-JjnpPwd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pure sampling Generation with GPT-2"
      ],
      "metadata": {
        "id": "-LVa51a8Q9bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "8RRuNxltP2pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top-k sampling\n",
        "\n",
        "As a response to the problems seen with these sampling methods, researchers have come up with different methods. An example is top-K sampling, which was used in the released GPT-2 model. In this method we sample, but we do so from the top-K words rather than all the vocabulary. This excludes the less likely words that meant the texts were not meaningful.\n",
        "\n",
        "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n"
      ],
      "metadata": {
        "id": "K2aktugvP8vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "U_XY6jUUP69f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}