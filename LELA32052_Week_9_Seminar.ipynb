{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3946b723",
      "metadata": {
        "id": "3946b723"
      },
      "source": [
        "# LELA32052 Computational Linguistics Week 9\n",
        "\n",
        "This week we are going to take a look at part of speech tagging."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a56a86",
      "metadata": {
        "id": "96a56a86"
      },
      "source": [
        "## Tagged corpora\n",
        "In looking to understand part of speech tagging, it is useful to start by looking at some human (rather than machine) tagged data. NLTK contains a number of corpora. We can import a few of these as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "14cae1f9",
      "metadata": {
        "id": "14cae1f9",
        "outputId": "b239a134-8cc7-473c-c062-1bbc582e3d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package sinica_treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data] Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/indian.zip.\n",
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data] Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_cat.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "nltk.download('sinica_treebank')\n",
        "nltk.download('indian')\n",
        "nltk.download('conll2002')\n",
        "nltk.download('cess_cat')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "692f918a",
      "metadata": {
        "id": "692f918a",
        "outputId": "ba90280e-91a6-48a3-e250-49ce7111e120",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Fulton', 'NP-TL'),\n",
              " ('County', 'NN-TL'),\n",
              " ('Grand', 'JJ-TL'),\n",
              " ('Jury', 'NN-TL'),\n",
              " ('said', 'VBD'),\n",
              " ('Friday', 'NR'),\n",
              " ('an', 'AT'),\n",
              " ('investigation', 'NN'),\n",
              " ('of', 'IN'),\n",
              " (\"Atlanta's\", 'NP$'),\n",
              " ('recent', 'JJ'),\n",
              " ('primary', 'NN'),\n",
              " ('election', 'NN'),\n",
              " ('produced', 'VBD'),\n",
              " ('``', '``'),\n",
              " ('no', 'AT'),\n",
              " ('evidence', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('that', 'CS'),\n",
              " ('any', 'DTI'),\n",
              " ('irregularities', 'NNS'),\n",
              " ('took', 'VBD'),\n",
              " ('place', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "brown.tagged_words()[1:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "af0308ad",
      "metadata": {
        "id": "af0308ad",
        "outputId": "c194555b-04d5-4427-db2d-6bb531a360bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "abe6114d",
      "metadata": {
        "id": "abe6114d",
        "outputId": "5ec5922c-6b44-46ce-8466-39c7302d7e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Fulton', 'NOUN'),\n",
              " ('County', 'NOUN'),\n",
              " ('Grand', 'ADJ'),\n",
              " ('Jury', 'NOUN'),\n",
              " ('said', 'VERB'),\n",
              " ('Friday', 'NOUN'),\n",
              " ('an', 'DET'),\n",
              " ('investigation', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " (\"Atlanta's\", 'NOUN'),\n",
              " ('recent', 'ADJ'),\n",
              " ('primary', 'NOUN'),\n",
              " ('election', 'NOUN'),\n",
              " ('produced', 'VERB'),\n",
              " ('``', '.'),\n",
              " ('no', 'DET'),\n",
              " ('evidence', 'NOUN'),\n",
              " (\"''\", '.'),\n",
              " ('that', 'ADP'),\n",
              " ('any', 'DET'),\n",
              " ('irregularities', 'NOUN'),\n",
              " ('took', 'VERB'),\n",
              " ('place', 'NOUN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "brown.tagged_words(tagset=\"universal\")[1:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f1be8f20",
      "metadata": {
        "id": "f1be8f20",
        "outputId": "da36aa06-f410-479f-c061-edb6e074c3dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('一', 'Neu'), ('友情', 'Nad'), ('嘉珍', 'Nba'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "nltk.corpus.sinica_treebank.tagged_words() # Chinese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "77905320",
      "metadata": {
        "id": "77905320",
        "outputId": "91e37d32-e40c-4674-c526-2a85236f54bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "nltk.corpus.indian.tagged_words() # Bangla, Hindi, Marathi, and Telugu language data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "25e04ece",
      "metadata": {
        "id": "25e04ece",
        "outputId": "5af7a2dd-872e-4021-9df5-020b911dd577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Sao', 'NC'), ('Paulo', 'VMI'), ('(', 'Fpa'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "nltk.corpus.conll2002.tagged_words() # Spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "56c03845",
      "metadata": {
        "id": "56c03845",
        "outputId": "804f7ee0-21b3-4690-99bf-19172b7325f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('El', 'da0ms0'), ('Tribunal_Suprem', 'np0000o'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "nltk.corpus.cess_cat.tagged_words() # Catalan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "778e55a5",
      "metadata": {
        "id": "778e55a5"
      },
      "source": [
        "## Inspecting tagged corpora\n",
        "\n",
        "Inspecting human tagged corpora can be useful for both linguistic research and for building taggers. We can use the NLTK toolkit to do this.\n",
        "\n",
        "Most straightforwardly we can look at the frequency with which particular words are given a tag (we will return to this later when we come to build a tagger)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2930d972-3ac9-42b6-9c95-54b913a9ccd5",
      "metadata": {
        "id": "2930d972-3ac9-42b6-9c95-54b913a9ccd5"
      },
      "outputs": [],
      "source": [
        "sent = [(\"the\",\"DET\"),(\"man\",\"NOUN\"),(\"walked\",\"VERB\"),(\"the\",\"DET\"),(\"dog\",\"NOUN\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0ab8f8e9",
      "metadata": {
        "id": "0ab8f8e9",
        "outputId": "9cbc2d74-ad81-46bc-d81f-295709802170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'DET': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "cfd1 = nltk.ConditionalFreqDist(sent)\n",
        "cfd1['the']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65b97e58",
      "metadata": {
        "id": "65b97e58"
      },
      "source": [
        "When we apply this to whole corpora, it becomes useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "88ffeff5",
      "metadata": {
        "id": "88ffeff5",
        "outputId": "300ecb3f-f837-4b68-d351-f6b07b5e1347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'DET': 62710, 'X': 3})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "brown_tagged = brown.tagged_words(tagset='universal')\n",
        "cfd1 = nltk.ConditionalFreqDist(brown_tagged)\n",
        "cfd1['the']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94cf95d7",
      "metadata": {
        "id": "94cf95d7"
      },
      "source": [
        "And if we additionally use a couple of other NLTK tools (which we don't have time to cover in detail - I just want to give you a sense of what is possible), we can look at the frequency with which particular word classes precede particular words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "33f3dd11",
      "metadata": {
        "id": "33f3dd11",
        "outputId": "7ad78717-4c25-48ac-db3b-ad4fb88c2164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   . VERB  ADP NOUN CONJ PRON  ADV  PRT  DET  ADJ \n",
            "  83   64   44   24   19   15   13    4    2    2 \n"
          ]
        }
      ],
      "source": [
        "brown_tagged = brown.tagged_words(tagset='universal')\n",
        "tags = [b[1] for (a, b) in nltk.bigrams(brown_tagged) if a[0] == 'car']\n",
        "fd = nltk.FreqDist(tags)\n",
        "fd.tabulate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97364b39",
      "metadata": {
        "id": "97364b39"
      },
      "source": [
        "Or the frequency with which particular word classes precede other word classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9606fc8c",
      "metadata": {
        "id": "9606fc8c",
        "outputId": "2d474a23-6fba-4507-92f4-1566eef2d19e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('DET', 85845),\n",
              " ('ADJ', 54653),\n",
              " ('NOUN', 41309),\n",
              " ('ADP', 37418),\n",
              " ('.', 20084),\n",
              " ('VERB', 17851),\n",
              " ('CONJ', 9294),\n",
              " ('NUM', 5668),\n",
              " ('ADV', 1851),\n",
              " ('PRT', 1068),\n",
              " ('PRON', 440),\n",
              " ('X', 77)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "brown_tagged = brown.tagged_words(tagset='universal')\n",
        "word_tag_pairs = nltk.bigrams(brown_tagged)\n",
        "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'NOUN']\n",
        "noun_preceders_fd = nltk.FreqDist(noun_preceders)\n",
        "[(wt,_) for (wt, _) in noun_preceders_fd.most_common()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39882a7b",
      "metadata": {
        "id": "39882a7b"
      },
      "source": [
        "And you can even search for particular constructional patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "24260d86",
      "metadata": {
        "id": "24260d86",
        "outputId": "7a028881-7337-45d2-e011-6783a197f659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "studied and revised\n",
            "modernizing and improving\n",
            "operated and follow\n",
            "appointed and elected\n"
          ]
        }
      ],
      "source": [
        "for tagged_sent in brown.tagged_sents(categories=\"news\")[1:75]:\n",
        "    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(tagged_sent):\n",
        "        if (t1.startswith('V') and w2 == 'and' and t3.startswith('V')):\n",
        "            print(w1, w2, w3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c608cc7",
      "metadata": {
        "id": "7c608cc7"
      },
      "source": [
        "## Building an automatic tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e030b80",
      "metadata": {
        "id": "2e030b80"
      },
      "source": [
        "A very simple approach to automated tagging that actually works quite well is to find the most common tag for each word in a training corpus (as we did above) and just tag all occurences of each word with its most common tag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "29b34b23",
      "metadata": {
        "id": "29b34b23"
      },
      "outputs": [],
      "source": [
        "brown_tagged_sents = brown.tagged_sents(tagset='universal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2b57d2d2",
      "metadata": {
        "id": "2b57d2d2"
      },
      "outputs": [],
      "source": [
        "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "03719751",
      "metadata": {
        "id": "03719751",
        "outputId": "1587bfbb-bbd4-48ab-bc27-271666d6120b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 'DET'),\n",
              " ('cat', 'NOUN'),\n",
              " ('sat', 'VERB'),\n",
              " ('on', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('mat', 'NOUN')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "unigram_tagger.tag([\"the\",\"cat\",\"sat\",\"on\",\"the\",\"mat\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d53540c9",
      "metadata": {
        "id": "d53540c9"
      },
      "source": [
        "We can formally evaluate this by splitting our data into a training set and a testing set. We obtain the by-word tag frequencies from the training set and evaluate by tagging the test set and comparing our predicted tags to the human tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2459c935",
      "metadata": {
        "id": "2459c935",
        "outputId": "49bd0e51-1da8-4104-a4d3-f92a2f6db839",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-512c22525284>:5: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  unigram_tagger.evaluate(test_sents)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9156346262651662"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "training_set_size = int(len(brown_tagged_sents) * 0.9)\n",
        "train_sents = brown_tagged_sents[:training_set_size]\n",
        "test_sents = brown_tagged_sents[training_set_size:]\n",
        "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
        "unigram_tagger.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29a8eb1c",
      "metadata": {
        "id": "29a8eb1c"
      },
      "source": [
        "### Regular expression based tagging\n",
        "\n",
        "As a next step we want to use a more intelligent way to deal with words we haven't seen before, but making use of their orthography and/or morphology. Write regular expressions to classify words in this way and see if you can improve performance. I've added one example rule to get you started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "98cdfe3e",
      "metadata": {
        "id": "98cdfe3e"
      },
      "outputs": [],
      "source": [
        "patterns = [\n",
        "    (r'.*ing$', 'VERB'),\n",
        "      ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "16873f4e",
      "metadata": {
        "id": "16873f4e",
        "outputId": "7b1bfb53-2db8-4fd9-e627-bbd5048c7209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-0b50dce44473>:4: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  t2.evaluate(test_sents)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9409903396827393"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "t0 = nltk.DefaultTagger('NOUN')\n",
        "t1 = nltk.RegexpTagger(patterns, backoff=t0)\n",
        "t2 = nltk.UnigramTagger(train_sents, backoff=t1)\n",
        "t2.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39fc9617",
      "metadata": {
        "id": "39fc9617"
      },
      "source": [
        "As with other classification tasks we can generate a confusion matrix to see where things are going right or wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "631b8865",
      "metadata": {
        "id": "631b8865",
        "outputId": "2c65dfdd-41d2-4255-f6b4-6e0eef2af1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "index cannot be a set",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6150a148c9ac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'editorial'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'editorial'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"universal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;31m# GH47215\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"index cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"columns cannot be a set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: index cannot be a set"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "predicted = [tag for sent in brown.sents(categories='editorial') for (word, tag) in t2.tag(sent)]\n",
        "true = [tag for (word, tag) in brown.tagged_words(categories='editorial',tagset=\"universal\")]\n",
        "print(pd.DataFrame(confusion_matrix(predicted, true),index=set(predicted)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looking at the context"
      ],
      "metadata": {
        "id": "o3xO_L2jriQP"
      },
      "id": "o3xO_L2jriQP"
    },
    {
      "cell_type": "markdown",
      "id": "5ffc87c6",
      "metadata": {
        "id": "5ffc87c6"
      },
      "source": [
        "We want to improve this, and an obvious next step is to give the tag that is most frequent for this word when it follows the previous word. The problem is this doesn't do very well. Any idea why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "d6929606",
      "metadata": {
        "id": "d6929606",
        "outputId": "39c3c9b1-7d71-46e7-9d83-0588dea19ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-9eaec05e914a>:2: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  bigram_tagger.evaluate(test_sents)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4737746484776094"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_sents)\n",
        "bigram_tagger.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cbf4f57",
      "metadata": {
        "id": "1cbf4f57"
      },
      "source": [
        "We can still make use of the bigram information by combining it with the unigram tagger via a process known as backing off - for each word we check whether we have seen that word and preceding word in our training data. If we have then we tag it with the most frequent tag for that word in that context. If we haven't seen it then we tag the word with its most frequent tag regardless of context. And if we haven't seen the word before we tag it as a noun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d4f85628",
      "metadata": {
        "id": "d4f85628",
        "outputId": "59a346a7-ff54-4e70-bcea-b59ae5f9d9e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-2caf3c855b29>:4: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  t2.evaluate(test_sents)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9485446658703716"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "t0 = nltk.DefaultTagger('NOUN')\n",
        "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
        "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
        "t2.evaluate(test_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e9b152",
      "metadata": {
        "id": "a5e9b152"
      },
      "source": [
        "### NLTK's Averaged Perceptron tagger\n",
        "\n",
        "NLTKs default prebuilt tagger uses a Perceptron just like that we have been using for other tasks on the module. For more information on this approach see here: https://explosion.ai/blog/part-of-speech-pos-tagger-in-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0936e1c4",
      "metadata": {
        "id": "0936e1c4",
        "outputId": "c0c70d9a-499a-4880-87df-0f1827c9cd58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "120b641d",
      "metadata": {
        "id": "120b641d"
      },
      "source": [
        "It can be run straightforwardly like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8337ddd3",
      "metadata": {
        "id": "8337ddd3",
        "outputId": "d2dc700b-699b-4fad-e7aa-b609fa93638a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ca81634493da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"And now for something completely different\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"universal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "text = nltk.word_tokenize(\"And now for something completely different\")\n",
        "nltk.pos_tag(text, tagset=\"universal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08864b1d",
      "metadata": {
        "id": "08864b1d"
      },
      "source": [
        "### POS tagging in other languages\n",
        "\n",
        "POS taggers are available for a great many languages. A popular package called Spacy contains a number. Here, as an example, is a German tagger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43868f13",
      "metadata": {
        "id": "43868f13"
      },
      "outputs": [],
      "source": [
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c260b97",
      "metadata": {
        "id": "9c260b97"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c39c35a",
      "metadata": {
        "id": "5c39c35a"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download de_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e1c5f40",
      "metadata": {
        "id": "1e1c5f40"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('de_core_news_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4569f4c2",
      "metadata": {
        "id": "4569f4c2"
      },
      "outputs": [],
      "source": [
        "text = \"Das ist nicht gut.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "487ceb42",
      "metadata": {
        "id": "487ceb42"
      },
      "outputs": [],
      "source": [
        "s1_t = nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05894816",
      "metadata": {
        "id": "05894816"
      },
      "outputs": [],
      "source": [
        "for tk in s1_t:\n",
        "    print(tk.text, tk.tag_, tk.pos_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqQ6hQEziFSp"
      },
      "source": [
        "### Chunking / Shallow Parsing\n",
        "\n",
        "Chunking involves grouping together words into elementary phrases. In its most common form it doesn't involve any hierachical structure.\n"
      ],
      "id": "yqQ6hQEziFSp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU7Nm8nxWtP3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ],
      "id": "iU7Nm8nxWtP3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvmAzVJnh4It"
      },
      "outputs": [],
      "source": [
        "text = nltk.word_tokenize(\"I study Linguistics and Social Anthropology at the University of Manchester\")"
      ],
      "id": "PvmAzVJnh4It"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zDIdnUEWIFJ"
      },
      "outputs": [],
      "source": [
        "grammar = r\"\"\"\n",
        "  NP: {<DET|ADP>?<ADJ>*<NOUN>}\n",
        "      {<NOUN>+}\n",
        "\"\"\"\n",
        "sent=nltk.pos_tag(text,tagset=\"universal\")\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "cs = cp.parse(sent)\n",
        "print(cs)"
      ],
      "id": "5zDIdnUEWIFJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e53dKlaEg73e"
      },
      "source": [
        "Update the grammar so that it produces the following shallow parse: <br> <br>\n",
        "(S <br>\n",
        "  (NP I/PRON) <br>\n",
        "  study/VERB <br>\n",
        "  (NP Linguistics/NOUN and/CONJ Social/NOUN Anthropology/NOUN) <br>\n",
        "  at/ADP <br>\n",
        "  (NP the/DET University/NOUN of/ADP Manchester/NOUN)) <br>"
      ],
      "id": "e53dKlaEg73e"
    }
  ],
  "metadata": {
    "colab": {
      "name": "Week_9_Seminar.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}